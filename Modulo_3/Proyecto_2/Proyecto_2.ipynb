{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Índice de Calidad del Aire</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.aire.cdmx.gob.mx/estadisticas-consultas/descargas/INDICExls-csv.pdf\n",
    "\n",
    "http://www.aire.cdmx.gob.mx/descargas/monitoreo/normatividad/NADF-009-AIRE-2017.pdf\n",
    "\n",
    "http://www.aire.cdmx.gob.mx/default.php?opc=%27aqBjnmU=%27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't know how to reset  #, please run `%reset?` for details\n",
      "Don't know how to reset  elimina, please run `%reset?` for details\n",
      "Don't know how to reset  todas, please run `%reset?` for details\n",
      "Don't know how to reset  las, please run `%reset?` for details\n",
      "Don't know how to reset  variables, please run `%reset?` for details\n",
      "Don't know how to reset  del, please run `%reset?` for details\n",
      "Don't know how to reset  entorno, please run `%reset?` for details\n"
     ]
    }
   ],
   "source": [
    "%reset -f # Elimina todas las variables del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install missingno \n",
    "# Libreria para la visualización de los valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Seleccionar las variables de interes y unir los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Hora</th>\n",
       "      <th>Noroeste PM10</th>\n",
       "      <th>Noroeste monóxido de carbono</th>\n",
       "      <th>Noreste PM10</th>\n",
       "      <th>Noreste monóxido de carbono</th>\n",
       "      <th>Centro PM10</th>\n",
       "      <th>Centro monóxido de carbono</th>\n",
       "      <th>Suroeste PM10</th>\n",
       "      <th>Suroeste monóxido de carbono</th>\n",
       "      <th>Sureste PM10</th>\n",
       "      <th>Sureste monóxido de carbono</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "      <td>13.0</td>\n",
       "      <td>117</td>\n",
       "      <td>16.0</td>\n",
       "      <td>107</td>\n",
       "      <td>16.0</td>\n",
       "      <td>67</td>\n",
       "      <td>9.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>13.0</td>\n",
       "      <td>122</td>\n",
       "      <td>18.0</td>\n",
       "      <td>107</td>\n",
       "      <td>17.0</td>\n",
       "      <td>70</td>\n",
       "      <td>9.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>3</td>\n",
       "      <td>113</td>\n",
       "      <td>14.0</td>\n",
       "      <td>124</td>\n",
       "      <td>19.0</td>\n",
       "      <td>109</td>\n",
       "      <td>18.0</td>\n",
       "      <td>76</td>\n",
       "      <td>10.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>4</td>\n",
       "      <td>115</td>\n",
       "      <td>14.0</td>\n",
       "      <td>126</td>\n",
       "      <td>19.0</td>\n",
       "      <td>110</td>\n",
       "      <td>19.0</td>\n",
       "      <td>83</td>\n",
       "      <td>10.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>15.0</td>\n",
       "      <td>127</td>\n",
       "      <td>20.0</td>\n",
       "      <td>112</td>\n",
       "      <td>17.0</td>\n",
       "      <td>90</td>\n",
       "      <td>11.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Fecha  Hora  Noroeste PM10  Noroeste monóxido de carbono  \\\n",
       "0  01/01/2016     1            108                          13.0   \n",
       "1  01/01/2016     2            110                          13.0   \n",
       "2  01/01/2016     3            113                          14.0   \n",
       "3  01/01/2016     4            115                          14.0   \n",
       "4  01/01/2016     5            116                          15.0   \n",
       "\n",
       "   Noreste PM10  Noreste monóxido de carbono  Centro PM10  \\\n",
       "0           117                         16.0          107   \n",
       "1           122                         18.0          107   \n",
       "2           124                         19.0          109   \n",
       "3           126                         19.0          110   \n",
       "4           127                         20.0          112   \n",
       "\n",
       "   Centro monóxido de carbono  Suroeste PM10  Suroeste monóxido de carbono  \\\n",
       "0                        16.0             67                           9.0   \n",
       "1                        17.0             70                           9.0   \n",
       "2                        18.0             76                          10.0   \n",
       "3                        19.0             83                          10.0   \n",
       "4                        17.0             90                          11.0   \n",
       "\n",
       "   Sureste PM10  Sureste monóxido de carbono  \n",
       "0         107.0                         11.0  \n",
       "1         107.0                         12.0  \n",
       "2         108.0                         13.0  \n",
       "3         107.0                         14.0  \n",
       "4         108.0                         14.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define la ruta de la carpeta donde están los archivos CSV\n",
    "# La ruta es una cadena de texto cruda (raw string) que permite incluir barras invertidas sin que se interpreten como caracteres de escape\n",
    "carpeta = r'./Original_dataset'\n",
    "\n",
    "# Lista para almacenar los DataFrames individuales\n",
    "# Aquí se va a ir guardando cada DataFrame que se genere a partir de los archivos CSV\n",
    "dataframes = []\n",
    "\n",
    "# Leer y almacenar las columnas relevantes de cada archivo CSV\n",
    "# Se recorre cada archivo en la carpeta especificada\n",
    "for archivo in os.listdir(carpeta):\n",
    "    # Verifica que el archivo termine en '.csv' para asegurarse de que estamos leyendo solo archivos CSV\n",
    "    if archivo.endswith('.csv'):\n",
    "        # Leer el archivo CSV\n",
    "        # Utiliza pandas para leer el archivo CSV y almacenarlo en un DataFrame\n",
    "        df = pd.read_csv(os.path.join(carpeta, archivo))\n",
    "        \n",
    "        # Seleccionar las columnas relevantes\n",
    "        # Se define una lista con las columnas que queremos mantener del DataFrame original\n",
    "        columnas_relevantes = ['Fecha', 'Hora']\n",
    "        # Añadimos las columnas específicas de cada zona que queremos conservar\n",
    "        for zona in ['Noroeste', 'Noreste', 'Centro', 'Suroeste', 'Sureste']:\n",
    "            columnas_relevantes.append(f'{zona} PM10')\n",
    "            columnas_relevantes.append(f'{zona} monóxido de carbono')\n",
    "        \n",
    "        # Filtrar el DataFrame para conservar solo las columnas relevantes\n",
    "        # Creamos un nuevo DataFrame que solo contiene las columnas especificadas en la lista 'columnas_relevantes'\n",
    "        df_relevante = df[columnas_relevantes]\n",
    "        \n",
    "        # Añadir el DataFrame filtrado a la lista\n",
    "        # Se agrega el DataFrame filtrado a la lista 'dataframes'\n",
    "        dataframes.append(df_relevante)\n",
    "\n",
    "# Combinar todos los DataFrames en uno solo\n",
    "# Utilizamos pandas para concatenar todos los DataFrames de la lista 'dataframes' en un único DataFrame\n",
    "data_combinada = pd.concat(dataframes, ignore_index=True)\n",
    "# Concatenar por columnas result = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "# Guardar el DataFrame combinado en un nuevo archivo CSV\n",
    "# Guardamos el DataFrame combinado en un archivo CSV llamado 'data_combinada.csv'\n",
    "data_combinada.to_csv('data_combinada.csv', index=False)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame combinado\n",
    "# Mostramos las primeras filas del DataFrame combinado para verificar que se ha creado correctamente\n",
    "data_combinada.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import missingno as msno \n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: missingno in c:\\users\\pc\\anaconda3\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\pc\\anaconda3\\lib\\site-packages (from missingno) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\pc\\anaconda3\\lib\\site-packages (from missingno) (3.8.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\pc\\anaconda3\\lib\\site-packages (from missingno) (1.11.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\pc\\anaconda3\\lib\\site-packages (from missingno) (0.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (2.8.2)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from seaborn->missingno) (2.1.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pandas>=0.25->seaborn->missingno) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pandas>=0.25->seaborn->missingno) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->missingno) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in c:\\users\\pc\\anaconda3\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.18 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from statsmodels) (1.26.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from statsmodels) (1.11.4)\n",
      "Requirement already satisfied: pandas>=1.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from statsmodels) (2.1.4)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from statsmodels) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from statsmodels) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pandas>=1.0->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pandas>=1.0->statsmodels) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pandas>=1.0->statsmodels) (2023.3)\n",
      "Requirement already satisfied: six in c:\\users\\pc\\anaconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hora</th>\n",
       "      <th>Noroeste PM10</th>\n",
       "      <th>Noroeste monóxido de carbono</th>\n",
       "      <th>Noreste PM10</th>\n",
       "      <th>Noreste monóxido de carbono</th>\n",
       "      <th>Centro PM10</th>\n",
       "      <th>Centro monóxido de carbono</th>\n",
       "      <th>Suroeste PM10</th>\n",
       "      <th>Suroeste monóxido de carbono</th>\n",
       "      <th>Sureste PM10</th>\n",
       "      <th>Sureste monóxido de carbono</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>69323.0</td>\n",
       "      <td>69323.0</td>\n",
       "      <td>69264.0</td>\n",
       "      <td>69323.0</td>\n",
       "      <td>69264.0</td>\n",
       "      <td>69323.0</td>\n",
       "      <td>69264.0</td>\n",
       "      <td>69323.0</td>\n",
       "      <td>69264.0</td>\n",
       "      <td>69313.0</td>\n",
       "      <td>69264.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Hora  Noroeste PM10  Noroeste monóxido de carbono  Noreste PM10  \\\n",
       "count  69323.0        69323.0                       69264.0       69323.0   \n",
       "mean      12.0           68.0                           2.0          75.0   \n",
       "std        7.0           25.0                          20.0          32.0   \n",
       "min        1.0          -99.0                         -99.0         -99.0   \n",
       "25%        7.0           48.0                           4.0          51.0   \n",
       "50%       12.0           65.0                           5.0          77.0   \n",
       "75%       18.0           88.0                           7.0         102.0   \n",
       "max       24.0          142.0                          43.0         166.0   \n",
       "\n",
       "       Noreste monóxido de carbono  Centro PM10  Centro monóxido de carbono  \\\n",
       "count                      69264.0      69323.0                     69264.0   \n",
       "mean                           3.0         58.0                         3.0   \n",
       "std                           20.0         24.0                        20.0   \n",
       "min                          -99.0        -99.0                       -99.0   \n",
       "25%                            4.0         40.0                         4.0   \n",
       "50%                            5.0         55.0                         5.0   \n",
       "75%                            8.0         74.0                         8.0   \n",
       "max                           64.0        140.0                        52.0   \n",
       "\n",
       "       Suroeste PM10  Suroeste monóxido de carbono  Sureste PM10  \\\n",
       "count        69323.0                       69264.0       69313.0   \n",
       "mean            42.0                           1.0          58.0   \n",
       "std             20.0                          20.0          41.0   \n",
       "min            -99.0                         -99.0         -99.0   \n",
       "25%             30.0                           3.0          40.0   \n",
       "50%             40.0                           4.0          60.0   \n",
       "75%             52.0                           5.0          84.0   \n",
       "max            115.0                          25.0         132.0   \n",
       "\n",
       "       Sureste monóxido de carbono  \n",
       "count                      69264.0  \n",
       "mean                           2.0  \n",
       "std                           20.0  \n",
       "min                          -99.0  \n",
       "25%                            4.0  \n",
       "50%                            5.0  \n",
       "75%                            7.0  \n",
       "max                           36.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el DataFrame\n",
    "df = pd.read_csv('data_combinada.csv')\n",
    "df.describe().round() # Podemos redondear los valores de salida "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar valores -99 con NaN\n",
    "df.replace(-99, np.nan, inplace=True)\n",
    "\n",
    "# Asegurarse de que a partir de la segunda columna todos los valores sean numéricos\n",
    "for col in df.columns[1:]:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Convertir la columna 'Fecha' al formato específico\n",
    "df['Fecha'] = pd.to_datetime(df['Fecha'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# Verificar si hay fechas que no se pudieron convertir\n",
    "invalid_dates = df[df['Fecha'].isna()]\n",
    "if not invalid_dates.empty:\n",
    "    print(\"Fechas no convertidas correctamente:\")\n",
    "    print(invalid_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Formatear la columna 'Fecha' al formato 'YYYY-MM-DD'\n",
    "df['Fecha'] = df['Fecha'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Renombrar columnas\n",
    "short_column_names = {\n",
    "    'Noroeste PM10': 'NO-PM10',\n",
    "    'Noroeste monóxido de carbono': 'NO-CO',\n",
    "    'Noreste PM10': 'NE-PM10',\n",
    "    'Noreste monóxido de carbono': 'NE-CO',\n",
    "    'Centro PM10': 'CE-PM10',\n",
    "    'Centro monóxido de carbono': 'CE-CO',\n",
    "    'Suroeste PM10': 'SO-PM10',\n",
    "    'Suroeste monóxido de carbono': 'SO-CO',\n",
    "    'Sureste PM10': 'SE-PM10',\n",
    "    'Sureste monóxido de carbono': 'SE-CO'\n",
    "}\n",
    "df.rename(columns=short_column_names, inplace=True)\n",
    "\n",
    "# Corregir las horas inválidas\n",
    "df['Hora'] = df['Hora'] - 1\n",
    "\n",
    "# Crear columna Fecha_Hora\n",
    "df['Fecha_Hora'] = pd.to_datetime(df['Fecha'] + ' ' + df['Hora'].astype(str) + ':00:00')\n",
    "df.set_index('Fecha_Hora', inplace=True)\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Hora</th>\n",
       "      <th>NO-PM10</th>\n",
       "      <th>NO-CO</th>\n",
       "      <th>NE-PM10</th>\n",
       "      <th>NE-CO</th>\n",
       "      <th>CE-PM10</th>\n",
       "      <th>CE-CO</th>\n",
       "      <th>SO-PM10</th>\n",
       "      <th>SO-CO</th>\n",
       "      <th>SE-PM10</th>\n",
       "      <th>SE-CO</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fecha_Hora</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>113.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 03:00:00</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>115.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 04:00:00</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Fecha  Hora  NO-PM10  NO-CO  NE-PM10  NE-CO  \\\n",
       "Fecha_Hora                                                              \n",
       "2016-01-01 00:00:00  2016-01-01     0    108.0   13.0    117.0   16.0   \n",
       "2016-01-01 01:00:00  2016-01-01     1    110.0   13.0    122.0   18.0   \n",
       "2016-01-01 02:00:00  2016-01-01     2    113.0   14.0    124.0   19.0   \n",
       "2016-01-01 03:00:00  2016-01-01     3    115.0   14.0    126.0   19.0   \n",
       "2016-01-01 04:00:00  2016-01-01     4    116.0   15.0    127.0   20.0   \n",
       "\n",
       "                     CE-PM10  CE-CO  SO-PM10  SO-CO  SE-PM10  SE-CO  \n",
       "Fecha_Hora                                                           \n",
       "2016-01-01 00:00:00    107.0   16.0     67.0    9.0    107.0   11.0  \n",
       "2016-01-01 01:00:00    107.0   17.0     70.0    9.0    107.0   12.0  \n",
       "2016-01-01 02:00:00    109.0   18.0     76.0   10.0    108.0   13.0  \n",
       "2016-01-01 03:00:00    110.0   19.0     83.0   10.0    107.0   14.0  \n",
       "2016-01-01 04:00:00    112.0   17.0     90.0   11.0    108.0   14.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hora</th>\n",
       "      <th>NO-PM10</th>\n",
       "      <th>NO-CO</th>\n",
       "      <th>NE-PM10</th>\n",
       "      <th>NE-CO</th>\n",
       "      <th>CE-PM10</th>\n",
       "      <th>CE-CO</th>\n",
       "      <th>SO-PM10</th>\n",
       "      <th>SO-CO</th>\n",
       "      <th>SE-PM10</th>\n",
       "      <th>SE-CO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>69323.000000</td>\n",
       "      <td>69248.000000</td>\n",
       "      <td>66645.000000</td>\n",
       "      <td>68898.000000</td>\n",
       "      <td>66728.000000</td>\n",
       "      <td>69299.000000</td>\n",
       "      <td>66869.000000</td>\n",
       "      <td>69035.000000</td>\n",
       "      <td>66637.000000</td>\n",
       "      <td>66649.000000</td>\n",
       "      <td>66564.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.499834</td>\n",
       "      <td>68.102082</td>\n",
       "      <td>5.946163</td>\n",
       "      <td>75.792635</td>\n",
       "      <td>7.008842</td>\n",
       "      <td>58.192499</td>\n",
       "      <td>6.246975</td>\n",
       "      <td>42.509828</td>\n",
       "      <td>4.523988</td>\n",
       "      <td>64.173116</td>\n",
       "      <td>5.715236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.921911</td>\n",
       "      <td>24.447218</td>\n",
       "      <td>3.294260</td>\n",
       "      <td>28.829598</td>\n",
       "      <td>4.058057</td>\n",
       "      <td>23.352578</td>\n",
       "      <td>3.439650</td>\n",
       "      <td>17.345446</td>\n",
       "      <td>2.133077</td>\n",
       "      <td>26.483766</td>\n",
       "      <td>3.029051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Hora       NO-PM10         NO-CO       NE-PM10         NE-CO  \\\n",
       "count  69323.000000  69248.000000  66645.000000  68898.000000  66728.000000   \n",
       "mean      11.499834     68.102082      5.946163     75.792635      7.008842   \n",
       "std        6.921911     24.447218      3.294260     28.829598      4.058057   \n",
       "min        0.000000     11.000000      0.000000      9.000000      0.000000   \n",
       "25%        6.000000     48.000000      4.000000     51.000000      5.000000   \n",
       "50%       11.000000     65.000000      5.000000     77.000000      6.000000   \n",
       "75%       17.000000     88.000000      7.000000    102.000000      9.000000   \n",
       "max       23.000000    142.000000     43.000000    166.000000     64.000000   \n",
       "\n",
       "            CE-PM10         CE-CO       SO-PM10         SO-CO       SE-PM10  \\\n",
       "count  69299.000000  66869.000000  69035.000000  66637.000000  66649.000000   \n",
       "mean      58.192499      6.246975     42.509828      4.523988     64.173116   \n",
       "std       23.352578      3.439650     17.345446      2.133077     26.483766   \n",
       "min        9.000000      0.000000      6.000000      0.000000      4.000000   \n",
       "25%       40.000000      4.000000     30.000000      3.000000     43.000000   \n",
       "50%       55.000000      5.000000     40.000000      4.000000     63.000000   \n",
       "75%       74.000000      8.000000     52.000000      5.000000     86.000000   \n",
       "max      140.000000     52.000000    115.000000     25.000000    132.000000   \n",
       "\n",
       "              SE-CO  \n",
       "count  66564.000000  \n",
       "mean       5.715236  \n",
       "std        3.029051  \n",
       "min        0.000000  \n",
       "25%        4.000000  \n",
       "50%        5.000000  \n",
       "75%        7.000000  \n",
       "max       36.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fecha         0\n",
       "Hora          0\n",
       "NO-PM10      75\n",
       "NO-CO      2678\n",
       "NE-PM10     425\n",
       "NE-CO      2595\n",
       "CE-PM10      24\n",
       "CE-CO      2454\n",
       "SO-PM10     288\n",
       "SO-CO      2686\n",
       "SE-PM10    2674\n",
       "SE-CO      2759\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msno.matrix(df) # Visualización de los datos nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Descripción por año\n",
    "df['Año'] = df.index.year\n",
    "df['Mes'] = df.index.month\n",
    "\n",
    "# Calcular valores nulos por año\n",
    "nulos_por_año = df.isnull().groupby(df['Año']).sum()\n",
    "\n",
    "# Calcular valores nulos por mes para cada año\n",
    "nulos_por_mes = df.isnull().groupby([df['Año'], df['Mes']]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Hora</th>\n",
       "      <th>NO-PM10</th>\n",
       "      <th>NO-CO</th>\n",
       "      <th>NE-PM10</th>\n",
       "      <th>NE-CO</th>\n",
       "      <th>CE-PM10</th>\n",
       "      <th>CE-CO</th>\n",
       "      <th>SO-PM10</th>\n",
       "      <th>SO-CO</th>\n",
       "      <th>SE-PM10</th>\n",
       "      <th>SE-CO</th>\n",
       "      <th>Año</th>\n",
       "      <th>Mes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Año</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>366</td>\n",
       "      <td>0</td>\n",
       "      <td>366</td>\n",
       "      <td>0</td>\n",
       "      <td>366</td>\n",
       "      <td>0</td>\n",
       "      <td>366</td>\n",
       "      <td>31</td>\n",
       "      <td>366</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>370</td>\n",
       "      <td>67</td>\n",
       "      <td>363</td>\n",
       "      <td>0</td>\n",
       "      <td>367</td>\n",
       "      <td>46</td>\n",
       "      <td>362</td>\n",
       "      <td>883</td>\n",
       "      <td>369</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>364</td>\n",
       "      <td>179</td>\n",
       "      <td>364</td>\n",
       "      <td>0</td>\n",
       "      <td>364</td>\n",
       "      <td>0</td>\n",
       "      <td>364</td>\n",
       "      <td>719</td>\n",
       "      <td>370</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>316</td>\n",
       "      <td>21</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>307</td>\n",
       "      <td>36</td>\n",
       "      <td>307</td>\n",
       "      <td>314</td>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>374</td>\n",
       "      <td>43</td>\n",
       "      <td>373</td>\n",
       "      <td>5</td>\n",
       "      <td>375</td>\n",
       "      <td>22</td>\n",
       "      <td>379</td>\n",
       "      <td>321</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>343</td>\n",
       "      <td>76</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "      <td>253</td>\n",
       "      <td>107</td>\n",
       "      <td>297</td>\n",
       "      <td>120</td>\n",
       "      <td>353</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>318</td>\n",
       "      <td>39</td>\n",
       "      <td>330</td>\n",
       "      <td>0</td>\n",
       "      <td>291</td>\n",
       "      <td>48</td>\n",
       "      <td>348</td>\n",
       "      <td>201</td>\n",
       "      <td>368</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>19</td>\n",
       "      <td>72</td>\n",
       "      <td>29</td>\n",
       "      <td>204</td>\n",
       "      <td>75</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>10</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Fecha  Hora  NO-PM10  NO-CO  NE-PM10  NE-CO  CE-PM10  CE-CO  SO-PM10  \\\n",
       "Año                                                                          \n",
       "2016      0     0        0    366        0    366        0    366        0   \n",
       "2017      0     0        2    370       67    363        0    367       46   \n",
       "2018      0     0        0    364      179    364        0    364        0   \n",
       "2019      0     0       47    316       21    307        0    307       36   \n",
       "2020      0     0       26    374       43    373        5    375       22   \n",
       "2021      0     0        0    343       76    271        0    253      107   \n",
       "2022      0     0        0    318       39    330        0    291       48   \n",
       "2023      0     0        0    168        0    162       19     72       29   \n",
       "2024      0     0        0     59        0     59        0     59        0   \n",
       "\n",
       "      SO-CO  SE-PM10  SE-CO  Año  Mes  \n",
       "Año                                    \n",
       "2016    366       31    366    0    0  \n",
       "2017    362      883    369    0    0  \n",
       "2018    364      719    370    0    0  \n",
       "2019    307      314    301    0    0  \n",
       "2020    379      321    375    0    0  \n",
       "2021    297      120    353    0    0  \n",
       "2022    348      201    368    0    0  \n",
       "2023    204       75    198    0    0  \n",
       "2024     59       10     59    0    0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nulos_por_año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Hora</th>\n",
       "      <th>NO-PM10</th>\n",
       "      <th>NO-CO</th>\n",
       "      <th>NE-PM10</th>\n",
       "      <th>NE-CO</th>\n",
       "      <th>CE-PM10</th>\n",
       "      <th>CE-CO</th>\n",
       "      <th>SO-PM10</th>\n",
       "      <th>SO-CO</th>\n",
       "      <th>SE-PM10</th>\n",
       "      <th>SE-CO</th>\n",
       "      <th>Año</th>\n",
       "      <th>Mes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Año</th>\n",
       "      <th>Mes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2016</th>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2024</th>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Fecha  Hora  NO-PM10  NO-CO  NE-PM10  NE-CO  CE-PM10  CE-CO  \\\n",
       "Año  Mes                                                                \n",
       "2016 1        0     0        0     30        0     30        0     30   \n",
       "     2        0     0        0     30        0     30        0     30   \n",
       "     3        0     0        0     30        0     30        0     30   \n",
       "     4        0     0        0     30        0     30        0     30   \n",
       "     5        0     0        0     30        0     30        0     30   \n",
       "...         ...   ...      ...    ...      ...    ...      ...    ...   \n",
       "2024 2        0     0        0     28        0     28        0     28   \n",
       "     3        0     0        0      0        0      0        0      0   \n",
       "     4        0     0        0      0        0      0        0      0   \n",
       "     5        0     0        0      0        0      0        0      0   \n",
       "     6        0     0        0      0        0      0        0      0   \n",
       "\n",
       "          SO-PM10  SO-CO  SE-PM10  SE-CO  Año  Mes  \n",
       "Año  Mes                                            \n",
       "2016 1          0     30        0     30    0    0  \n",
       "     2          0     30        0     30    0    0  \n",
       "     3          0     30        5     30    0    0  \n",
       "     4          0     30        0     30    0    0  \n",
       "     5          0     30        0     30    0    0  \n",
       "...           ...    ...      ...    ...  ...  ...  \n",
       "2024 2          0     28        0     28    0    0  \n",
       "     3          0      0        0      0    0    0  \n",
       "     4          0      0        0      0    0    0  \n",
       "     5          0      0       10      0    0    0  \n",
       "     6          0      0        0      0    0    0  \n",
       "\n",
       "[95 rows x 14 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nulos_por_mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para graficar series de tiempo con rango de fechas y zonas específicas\n",
    "def plot_time_series(df, variable, zones, ylabel, title, start_date=None, end_date=None):\n",
    "    # Configurar el tamaño de la figura del gráfico\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Si se proporcionan fechas de inicio y fin, filtrar el DataFrame para ese rango de fechas\n",
    "    if start_date and end_date:\n",
    "        df = df.loc[start_date:end_date]\n",
    "    \n",
    "    # Iterar sobre cada zona especificada en la lista de zonas\n",
    "    for zone in zones:\n",
    "        # Crear el nombre de la columna combinando la zona y la variable\n",
    "        column = f\"{zone}-{variable}\"\n",
    "        \n",
    "        # Verificar si la columna existe en el DataFrame\n",
    "        if column in df.columns:\n",
    "            # Graficar la serie de tiempo de la columna\n",
    "            plt.plot(df.index, df[column], label=column)\n",
    "        else:\n",
    "            # Imprimir un mensaje si la columna no se encuentra en el DataFrame\n",
    "            print(f\"Columna {column} no encontrada en el DataFrame\")\n",
    "\n",
    "    # Etiqueta del eje x\n",
    "    plt.xlabel('Fecha_Hora')\n",
    "    # Etiqueta del eje y\n",
    "    plt.ylabel(ylabel)\n",
    "    # Título del gráfico\n",
    "    plt.title(title)\n",
    "    # Mostrar la leyenda del gráfico\n",
    "    plt.legend()\n",
    "    # Mostrar el gráfico\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las zonas específicas que se desean graficar para PM10\n",
    "zones_to_plot = ['NO', 'NE', 'CE']\n",
    "\n",
    "# Llamar a la función para graficar PM10 en las zonas específicas con el rango de fechas indicado\n",
    "plot_time_series(df, 'PM10', zones_to_plot, 'Índice (PM10)', 'Series de Tiempo de PM10 en Diferentes Zonas', start_date='2016-01-01', end_date='2016-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las zonas específicas que se desean graficar para PM10\n",
    "zones_to_plot = ['CE']\n",
    "\n",
    "# Llamar a la función para graficar PM10 en las zonas específicas con el rango de fechas indicado\n",
    "plot_time_series(df, 'PM10', zones_to_plot, 'Índice (PM10)', 'Series de Tiempo de PM10 en Diferentes Zonas', start_date=\"2016-01-01\", end_date=\"2016-01-05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las zonas específicas que se desean graficar para CO\n",
    "zones_to_plot = ['NO', 'NE', 'CE']\n",
    "\n",
    "# Llamar a la función para graficar CO en las zonas específicas con el rango de fechas indicado\n",
    "plot_time_series(df, 'CO', zones_to_plot, 'Índice (Monóxido de Carbono)', 'Series de Tiempo de Monóxido de Carbono en Diferentes Zonas', start_date='2016-01-01', end_date='2016-12-31')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar columnas para PM10 y CO\n",
    "pm10_columns = [col for col in df.columns if 'PM10' in col]\n",
    "co_columns = [col for col in df.columns if 'CO' in col]\n",
    "\n",
    "# Crear nuevas columnas para promedios\n",
    "df['MEAN-PM10'] = df[pm10_columns].mean(axis=1, skipna=True)\n",
    "df['MEAN-CO'] = df[co_columns].mean(axis=1, skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Hora</th>\n",
       "      <th>NO-PM10</th>\n",
       "      <th>NO-CO</th>\n",
       "      <th>NE-PM10</th>\n",
       "      <th>NE-CO</th>\n",
       "      <th>CE-PM10</th>\n",
       "      <th>CE-CO</th>\n",
       "      <th>SO-PM10</th>\n",
       "      <th>SO-CO</th>\n",
       "      <th>SE-PM10</th>\n",
       "      <th>SE-CO</th>\n",
       "      <th>Año</th>\n",
       "      <th>Mes</th>\n",
       "      <th>MEAN-PM10</th>\n",
       "      <th>MEAN-CO</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fecha_Hora</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>101.2</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>103.2</td>\n",
       "      <td>13.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>113.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>106.0</td>\n",
       "      <td>14.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 03:00:00</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>115.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>108.2</td>\n",
       "      <td>15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 04:00:00</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>110.6</td>\n",
       "      <td>15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-29 12:00:00</th>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>12</td>\n",
       "      <td>39.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>32.4</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-29 13:00:00</th>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>13</td>\n",
       "      <td>39.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>32.8</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-29 14:00:00</th>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>14</td>\n",
       "      <td>39.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>32.8</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-29 15:00:00</th>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>15</td>\n",
       "      <td>36.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>32.4</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-29 16:00:00</th>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>16</td>\n",
       "      <td>34.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>32.2</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69323 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Fecha  Hora  NO-PM10  NO-CO  NE-PM10  NE-CO  \\\n",
       "Fecha_Hora                                                              \n",
       "2016-01-01 00:00:00  2016-01-01     0    108.0   13.0    117.0   16.0   \n",
       "2016-01-01 01:00:00  2016-01-01     1    110.0   13.0    122.0   18.0   \n",
       "2016-01-01 02:00:00  2016-01-01     2    113.0   14.0    124.0   19.0   \n",
       "2016-01-01 03:00:00  2016-01-01     3    115.0   14.0    126.0   19.0   \n",
       "2016-01-01 04:00:00  2016-01-01     4    116.0   15.0    127.0   20.0   \n",
       "...                         ...   ...      ...    ...      ...    ...   \n",
       "2024-06-29 12:00:00  2024-06-29    12     39.0    8.0     39.0    7.0   \n",
       "2024-06-29 13:00:00  2024-06-29    13     39.0    8.0     39.0    7.0   \n",
       "2024-06-29 14:00:00  2024-06-29    14     39.0    7.0     39.0    6.0   \n",
       "2024-06-29 15:00:00  2024-06-29    15     36.0    6.0     39.0    6.0   \n",
       "2024-06-29 16:00:00  2024-06-29    16     34.0    6.0     39.0    6.0   \n",
       "\n",
       "                     CE-PM10  CE-CO  SO-PM10  SO-CO  SE-PM10  SE-CO   Año  \\\n",
       "Fecha_Hora                                                                  \n",
       "2016-01-01 00:00:00    107.0   16.0     67.0    9.0    107.0   11.0  2016   \n",
       "2016-01-01 01:00:00    107.0   17.0     70.0    9.0    107.0   12.0  2016   \n",
       "2016-01-01 02:00:00    109.0   18.0     76.0   10.0    108.0   13.0  2016   \n",
       "2016-01-01 03:00:00    110.0   19.0     83.0   10.0    107.0   14.0  2016   \n",
       "2016-01-01 04:00:00    112.0   17.0     90.0   11.0    108.0   14.0  2016   \n",
       "...                      ...    ...      ...    ...      ...    ...   ...   \n",
       "2024-06-29 12:00:00     35.0    8.0     23.0    7.0     26.0    6.0  2024   \n",
       "2024-06-29 13:00:00     35.0    8.0     23.0    7.0     28.0    6.0  2024   \n",
       "2024-06-29 14:00:00     35.0    7.0     23.0    6.0     28.0    6.0  2024   \n",
       "2024-06-29 15:00:00     35.0    7.0     24.0    6.0     28.0    5.0  2024   \n",
       "2024-06-29 16:00:00     34.0    6.0     24.0    6.0     30.0    5.0  2024   \n",
       "\n",
       "                     Mes  MEAN-PM10  MEAN-CO  \n",
       "Fecha_Hora                                    \n",
       "2016-01-01 00:00:00    1      101.2     13.0  \n",
       "2016-01-01 01:00:00    1      103.2     13.8  \n",
       "2016-01-01 02:00:00    1      106.0     14.8  \n",
       "2016-01-01 03:00:00    1      108.2     15.2  \n",
       "2016-01-01 04:00:00    1      110.6     15.4  \n",
       "...                  ...        ...      ...  \n",
       "2024-06-29 12:00:00    6       32.4      7.2  \n",
       "2024-06-29 13:00:00    6       32.8      7.2  \n",
       "2024-06-29 14:00:00    6       32.8      6.4  \n",
       "2024-06-29 15:00:00    6       32.4      6.0  \n",
       "2024-06-29 16:00:00    6       32.2      5.8  \n",
       "\n",
       "[69323 rows x 16 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_time_series(df, 'PM10',['MEAN'], 'Índice (PM10)', 'MEDIA PM10', start_date='2016-01-01', end_date='2016-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_averages(df, start_date, end_date, columns, freq='D'):\n",
    "    \"\"\"\n",
    "    Calcula los promedios de las zonas para las columnas especificadas en un rango de fechas específico.\n",
    "    \n",
    "    Parámetros:\n",
    "    df (DataFrame): El DataFrame con los datos.\n",
    "    start_date (str): Fecha de inicio en formato 'YYYY-MM-DD'.\n",
    "    end_date (str): Fecha de fin en formato 'YYYY-MM-DD'.\n",
    "    columns (list): Lista de columnas a promediar.\n",
    "    freq (str): Frecuencia para agrupar los datos ('D' para día, 'W' para semana, 'M' para mes).\n",
    "    \n",
    "    Retorna:\n",
    "    DataFrame: Un DataFrame con los promedios calculados.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtrar el DataFrame por el rango de fechas\n",
    "    df_filtered = df[(df.index >= start_date) & (df.index <= end_date)]\n",
    "\n",
    "    # Crear DataFrame vacío para almacenar los promedios\n",
    "    df_avg = pd.DataFrame()\n",
    "\n",
    "    # Agrupar por la frecuencia especificada y calcular los promedios\n",
    "    for col in columns:\n",
    "        # Filtrar las columnas del DataFrame que contienen el nombre de la variable especificada (col)\n",
    "        zone_columns = [c for c in df_filtered.columns if col in c]\n",
    "        # Resamplear (reagrupar) los datos según la frecuencia especificada y calcular la media\n",
    "        df_avg[f'{col}'] = df_filtered[zone_columns].resample(freq).mean().mean(axis=1, skipna=True)\n",
    "    \n",
    "    # Retornar el DataFrame con los promedios calculados\n",
    "    return df_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CE-CO</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fecha_Hora</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>12.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03</th>\n",
       "      <td>9.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>10.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>15.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-27</th>\n",
       "      <td>9.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-28</th>\n",
       "      <td>6.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29</th>\n",
       "      <td>6.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30</th>\n",
       "      <td>6.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>366 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                CE-CO\n",
       "Fecha_Hora           \n",
       "2016-01-01  12.625000\n",
       "2016-01-02   7.000000\n",
       "2016-01-03   9.208333\n",
       "2016-01-04  10.291667\n",
       "2016-01-05  15.111111\n",
       "...               ...\n",
       "2016-12-27   9.416667\n",
       "2016-12-28   6.958333\n",
       "2016-12-29   6.166667\n",
       "2016-12-30   6.833333\n",
       "2016-12-31  13.000000\n",
       "\n",
       "[366 rows x 1 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = 'CE-CO'\n",
    "df_avg = calculate_averages(df, start_date='2016-01-01', end_date='2016-12-31', columns=[col], freq='D')\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la media móvil (tendencia) con una ventana de 12 periodos para cada columna y agregarlo al DataFrame\n",
    "df_avg[f'{col}-trend'] = df_avg[col].rolling(window=12, min_periods=1).mean()\n",
    "\n",
    "# Configurar el tamaño de la figura del gráfico\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Graficar la serie de tiempo original de la columna\n",
    "plt.plot(df_avg.index, df_avg[col], label=col)\n",
    "\n",
    "# Graficar la serie de tiempo de la tendencia (media móvil) con una línea discontinua\n",
    "plt.plot(df_avg.index, df_avg[f'{col}-trend'], linestyle='--', label=f'{col} Trend')\n",
    "\n",
    "# Título del gráfico\n",
    "plt.title('Promedios por Periodo')\n",
    "\n",
    "# Etiqueta del eje x\n",
    "plt.xlabel('Fecha')\n",
    "\n",
    "# Etiqueta del eje y\n",
    "plt.ylabel('Promedio')\n",
    "\n",
    "# Mostrar la leyenda del gráfico\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular los promedios y desviaciones estándar\n",
    "def calculate_averages_and_std(df, start_date, end_date, columns, freq='D'):\n",
    "    \"\"\"\n",
    "    Calcula los promedios y desviaciones estándar de las zonas para las columnas especificadas en un rango de fechas específico.\n",
    "    \n",
    "    Parámetros:\n",
    "    df (DataFrame): El DataFrame con los datos.\n",
    "    start_date (str): Fecha de inicio en formato 'YYYY-MM-DD'.\n",
    "    end_date (str): Fecha de fin en formato 'YYYY-MM-DD'.\n",
    "    columns (list): Lista de columnas a promediar.\n",
    "    freq (str): Frecuencia para agrupar los datos ('D' para día, 'W' para semana, 'M' para mes).\n",
    "    \n",
    "    Retorna:\n",
    "    DataFrame: Un DataFrame con los promedios y desviaciones estándar calculados.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtrar el DataFrame por el rango de fechas\n",
    "    df_filtered = df[(df.index >= start_date) & (df.index <= end_date)]\n",
    "\n",
    "    # Crear DataFrames vacíos para almacenar los promedios y desviaciones estándar\n",
    "    df_avg = pd.DataFrame()\n",
    "    df_std = pd.DataFrame()\n",
    "\n",
    "    # Agrupar por la frecuencia especificada y calcular los promedios y desviaciones estándar\n",
    "    for col in columns:\n",
    "        # Filtrar las columnas del DataFrame que contienen el nombre de la variable especificada (col)\n",
    "        zone_columns = [c for c in df_filtered.columns if col in c]\n",
    "        # Resamplear (reagrupar) los datos según la frecuencia especificada y calcular la media\n",
    "        df_avg[f'{col}'] = df_filtered[zone_columns].resample(freq).mean().mean(axis=1, skipna=True)\n",
    "        # Resamplear (reagrupar) los datos según la frecuencia especificada y calcular la desviación estándar\n",
    "        df_std[f'{col}'] = df_filtered[zone_columns].resample(freq).std().mean(axis=1, skipna=True)\n",
    "    \n",
    "    # Retornar los DataFrames con los promedios y desviaciones estándar calculados\n",
    "    return df_avg, df_std\n",
    "\n",
    "# Definir la columna a analizar\n",
    "col = 'CE-CO'\n",
    "\n",
    "# Calcular los promedios y desviaciones estándar para la columna especificada en el rango de fechas y frecuencia dada\n",
    "df_avg, df_std = calculate_averages_and_std(df, start_date='2016-01-01', end_date='2018-12-31', columns=[col], freq='W')\n",
    "\n",
    "# Calcular la tendencia usando una media móvil con ventana de 5 periodos\n",
    "df_avg[f'{col}-trend'] = df_avg[col].rolling(window=5, min_periods=1).mean()\n",
    "\n",
    "# Configurar el tamaño de la figura del gráfico\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Graficar los promedios de la columna\n",
    "plt.plot(df_avg.index, df_avg[col], label=col)\n",
    "\n",
    "# Graficar la tendencia con una línea discontinua\n",
    "plt.plot(df_avg.index, df_avg[f'{col}-trend'], linestyle='--', label=f'{col} Trend')\n",
    "\n",
    "# Agregar banda de desviación estándar (área sombreada)\n",
    "plt.fill_between(df_avg.index, df_avg[col] - df_std[col], df_avg[col] + df_std[col], color='gray', alpha=0.2)\n",
    "\n",
    "# Título del gráfico\n",
    "plt.title('Promedios por Periodo con Desviación Estándar')\n",
    "\n",
    "# Etiqueta del eje x\n",
    "plt.xlabel('Fecha')\n",
    "\n",
    "# Etiqueta del eje y\n",
    "plt.ylabel('Promedio')\n",
    "\n",
    "# Mostrar la leyenda del gráfico\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomponer la serie temporal\n",
    "decomposition = seasonal_decompose(df_avg['CE-CO'], model='additive', period=12) #additive/aditivo, suponemos que se crea apartir de la suma de la tendencia. estacionalidad y residuales\n",
    "#multiplicative/multiplicativa, suponemos que se crea apartir de la multiplicación de la tendencia. estacionalidad y residuales\n",
    "\n",
    "# Graficar los componentes\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(df_avg['CE-CO'], label='Original')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Serie Temporal Original')\n",
    "\n",
    "plt.subplot(412)\n",
    "plt.plot(decomposition.trend, label='Tendencia')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Componente de Tendencia')\n",
    "\n",
    "plt.subplot(413)\n",
    "plt.plot(decomposition.seasonal, label='Estacionalidad')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Componente de Estacionalidad')\n",
    "\n",
    "plt.subplot(414)\n",
    "plt.plot(decomposition.resid, label='Residuales')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Componente Residual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hourly_boxplot(df, start_date, end_date, columns):\n",
    "    \"\"\"\n",
    "    Calcula los promedios y desviaciones estándar de las zonas para las columnas especificadas en un rango de fechas específico por hora y genera una gráfica de cajas y bigotes.\n",
    "    \n",
    "    Parámetros:\n",
    "    df (DataFrame): El DataFrame con los datos.\n",
    "    start_date (str): Fecha de inicio en formato 'YYYY-MM-DD'.\n",
    "    end_date (str): Fecha de fin en formato 'YYYY-MM-DD'.\n",
    "    columns (list): Lista de columnas a promediar.\n",
    "    \n",
    "    Retorna:\n",
    "    None: Genera una gráfica de cajas y bigotes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtrar el DataFrame por el rango de fechas\n",
    "    df_filtered = df[(df.index >= start_date) & (df.index <= end_date)]\n",
    "\n",
    "    # Crear un DataFrame solo con las columnas relevantes\n",
    "    df_hourly = df_filtered[['Hora'] + columns]\n",
    "\n",
    "    # Derretir el DataFrame para que sea apto para Seaborn\n",
    "    df_hourly_melted = df_hourly.melt(id_vars=['Hora'], value_vars=columns, var_name='Variable', value_name='Valor')\n",
    "\n",
    "    # Calcular la media por hora y variable\n",
    "    df_means = df_hourly.groupby('Hora')[columns].mean().reset_index()\n",
    "\n",
    "    # Graficar con Seaborn\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.boxplot(x='Hora', y='Valor', data=df_hourly_melted, showfliers=True)\n",
    "    \n",
    "    # Añadir la línea de media al gráfico\n",
    "    for col in columns:\n",
    "        sns.lineplot(x='Hora', y=col, data=df_means, label=f'Media {col}' , color = 'r')\n",
    "\n",
    "    plt.title('Concentración por hora')\n",
    "    plt.xlabel('Hora del día')\n",
    "    plt.ylabel('Concentración')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    }
   ],
   "source": [
    "calculate_hourly_boxplot(df, start_date='2016-01-01', end_date='2016-12-31', columns=['CE-CO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weekly_boxplot(df, start_date, end_date, columns):\n",
    "    \"\"\"\n",
    "    Calcula los promedios y desviaciones estándar de las zonas para las columnas especificadas en un rango de fechas específico por día de la semana y genera una gráfica de cajas y bigotes.\n",
    "    \n",
    "    Parámetros:\n",
    "    df (DataFrame): El DataFrame con los datos.\n",
    "    start_date (str): Fecha de inicio en formato 'YYYY-MM-DD'.\n",
    "    end_date (str): Fecha de fin en formato 'YYYY-MM-DD'.\n",
    "    columns (list): Lista de columnas a promediar.\n",
    "    \n",
    "    Retorna:\n",
    "    None: Genera una gráfica de cajas y bigotes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtrar el DataFrame por el rango de fechas\n",
    "    df_filtered = df[(df.index >= start_date) & (df.index <= end_date)].copy()\n",
    "\n",
    "    # Agregar columna de día de la semana\n",
    "    df_filtered['Día de la semana'] = df_filtered.index.day_name()\n",
    "\n",
    "    # Crear un DataFrame solo con las columnas relevantes\n",
    "    df_weekly = df_filtered[['Día de la semana'] + columns]\n",
    "\n",
    "    # Derretir el DataFrame para que sea apto para Seaborn\n",
    "    df_weekly_melted = df_weekly.melt(id_vars=['Día de la semana'], value_vars=columns, var_name='Variable', value_name='Valor')\n",
    "\n",
    "    # Calcular la media por día de la semana y variable\n",
    "    df_means = df_weekly.groupby('Día de la semana')[columns].mean().reset_index()\n",
    "\n",
    "    # Ordenar los días de la semana\n",
    "    order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    df_means['Día de la semana'] = pd.Categorical(df_means['Día de la semana'], categories=order, ordered=True)\n",
    "    df_means = df_means.sort_values('Día de la semana')\n",
    "\n",
    "    # Graficar con Seaborn\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.boxplot(x='Día de la semana', y='Valor', data=df_weekly_melted, order=order, showfliers=False)\n",
    "\n",
    "    # Añadir la línea de media al gráfico\n",
    "    for col in columns:\n",
    "        sns.lineplot(x='Día de la semana', y=col, data=df_means, label=f'Media {col}',color='r')\n",
    "\n",
    "    plt.title('Concentración por día de la semana')\n",
    "    plt.xlabel('Día de la semana')\n",
    "    plt.ylabel('Concentración')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    }
   ],
   "source": [
    "calculate_weekly_boxplot(df, start_date='2016-01-01', end_date='2016-12-31', columns=['CE-CO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\pc\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    }
   ],
   "source": [
    "calculate_weekly_boxplot(df, start_date='2021-01-01', end_date='2021-12-31', columns=['CE-CO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los intervalos y colores según las tablas proporcionadas\n",
    "intervalos = [\n",
    "    (0, 50, '#9ACA3C'),     # Intervalo 0-50 con color verde\n",
    "    (51, 100, '#F7EC0F'),   # Intervalo 51-100 con color amarillo\n",
    "    (101, 150, '#F8991D'),  # Intervalo 101-150 con color naranja\n",
    "    (151, 200, '#ED2124'),  # Intervalo 151-200 con color rojo\n",
    "    (201, 300, '#7D287D'),  # Intervalo 201-300 con color morado\n",
    "    (301, 500, '#7E0023')   # Intervalo 301-500 con color marrón oscuro\n",
    "]\n",
    "\n",
    "# Función para obtener el color basado en el valor\n",
    "def obtener_color(valor):\n",
    "    for intervalo in intervalos:\n",
    "        if intervalo[0] <= valor <= intervalo[1]:\n",
    "            return intervalo[2]\n",
    "    return '#000000'  # Negro por defecto si el valor no encaja en ningún intervalo\n",
    "\n",
    "# Función para graficar series de tiempo con puntos coloreados según los intervalos\n",
    "def plot_time_series_points(df, variable, zones, ylabel, title, start_date=None, end_date=None):\n",
    "    # Configurar el tamaño de la figura del gráfico\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Filtrar el DataFrame por el rango de fechas si se proporcionan fechas de inicio y fin\n",
    "    if start_date and end_date:\n",
    "        df = df.loc[start_date:end_date]\n",
    "    \n",
    "    # Iterar sobre cada zona especificada en la lista de zonas\n",
    "    for zone in zones:\n",
    "        # Crear el nombre de la columna combinando la zona y la variable\n",
    "        column = f\"{zone}-{variable}\"\n",
    "        \n",
    "        # Verificar si la columna existe en el DataFrame\n",
    "        if (column in df.columns):\n",
    "            # Aplicar la función obtener_color a cada valor de la columna para obtener los colores correspondientes\n",
    "            colors = df[column].apply(obtener_color)\n",
    "            # Graficar los puntos con los colores obtenidos\n",
    "            plt.scatter(df.index, df[column], c=colors, label=column)\n",
    "        else:\n",
    "            # Imprimir un mensaje si la columna no se encuentra en el DataFrame\n",
    "            print(f\"Columna {column} no encontrada en el DataFrame\")\n",
    "\n",
    "    # Etiqueta del eje x\n",
    "    plt.xlabel('Fecha_Hora')\n",
    "    # Etiqueta del eje y\n",
    "    plt.ylabel(ylabel)\n",
    "    # Título del gráfico\n",
    "    plt.title(title)\n",
    "    # Mostrar la leyenda del gráfico\n",
    "    plt.legend()\n",
    "    # Mostrar el gráfico\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_to_plot = ['CE']\n",
    "plot_time_series_points(df, 'PM10', zones_to_plot, 'Índice (PM10)', '', start_date='2016-01-01', end_date='2024-12-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Imputación de valores faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del DataFrame filtrado:\n",
      "                     CE-CO  CE-PM10\n",
      "Fecha_Hora                         \n",
      "2016-01-01 00:00:00   16.0    107.0\n",
      "2016-01-01 01:00:00   17.0    107.0\n",
      "2016-01-01 02:00:00   18.0    109.0\n",
      "2016-01-01 03:00:00   19.0    110.0\n",
      "2016-01-01 04:00:00   17.0    112.0\n",
      "\n",
      "Número de valores nulos antes de la imputación:\n",
      "CE-CO      1224\n",
      "CE-PM10       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Filtrar el DataFrame para obtener las columnas 'CE-CO' y 'CE-PM10' en el rango de fechas especificado\n",
    "df_impute = df[['CE-CO', 'CE-PM10']].loc['2016-01-01':'2019-05-01']\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame filtrado\n",
    "print(\"Primeras filas del DataFrame filtrado:\")\n",
    "print(df_impute.head())\n",
    "\n",
    "# Graficar las series de tiempo de 'CE-CO' y 'CE-PM10'\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_impute.plot()\n",
    "plt.title('Series de Tiempo de CE-CO y CE-PM10')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Concentración')\n",
    "plt.show()\n",
    "\n",
    "# Contar y mostrar el número de valores nulos en cada columna del DataFrame filtrado\n",
    "print(\"\\nNúmero de valores nulos antes de la imputación:\")\n",
    "print(df_impute.isnull().sum())\n",
    "\n",
    "# Crear una instancia del imputador KNN con 12 vecinos\n",
    "imputer_knn = KNNImputer(n_neighbors=12)\n",
    "\n",
    "# Aplicar el imputador KNN para llenar los valores nulos en el DataFrame\n",
    "df_impute.loc[:, :] = imputer_knn.fit_transform(df_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras filas del DataFrame después de la imputación y adición de columnas:\n",
      "                     CE-CO  CE-PM10  julian_day  hour\n",
      "Fecha_Hora                                           \n",
      "2016-01-01 00:00:00   16.0    107.0           1     0\n",
      "2016-01-01 01:00:00   17.0    107.0           1     1\n",
      "2016-01-01 02:00:00   18.0    109.0           1     2\n",
      "2016-01-01 03:00:00   19.0    110.0           1     3\n",
      "2016-01-01 04:00:00   17.0    112.0           1     4\n",
      "\n",
      "Número de valores nulos después de la imputación:\n",
      "CE-CO         0\n",
      "CE-PM10       0\n",
      "julian_day    0\n",
      "hour          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Agregar una columna con el día juliano (día del año)\n",
    "df_impute['julian_day'] = df_impute.index.dayofyear\n",
    "\n",
    "# Agregar una columna con la hora del día\n",
    "df_impute['hour'] = df_impute.index.hour\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame después de la imputación y adición de columnas\n",
    "print(\"\\nPrimeras filas del DataFrame después de la imputación y adición de columnas:\")\n",
    "print(df_impute.head())\n",
    "\n",
    "# Contar y mostrar el número de valores nulos después de la imputación\n",
    "print(\"\\nNúmero de valores nulos después de la imputación:\")\n",
    "print(df_impute.isnull().sum())\n",
    "\n",
    "# Graficar las series de tiempo de 'CE-CO' y 'CE-PM10' después de la imputación\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_impute[['CE-CO', 'CE-PM10']].plot()\n",
    "plt.title('Series de Tiempo de CE-CO y CE-PM10 (Después de la Imputación)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Concentración')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del DataFrame filtrado:\n",
      "                     CE-CO  CE-PM10\n",
      "Fecha_Hora                         \n",
      "2016-01-01 00:00:00   16.0    107.0\n",
      "2016-01-01 01:00:00   17.0    107.0\n",
      "2016-01-01 02:00:00   18.0    109.0\n",
      "2016-01-01 03:00:00   19.0    110.0\n",
      "2016-01-01 04:00:00   17.0    112.0\n",
      "\n",
      "Número de valores nulos antes de la imputación:\n",
      "CE-CO      1224\n",
      "CE-PM10       0\n",
      "dtype: int64\n",
      "\n",
      "Primeras filas del DataFrame después de la interpolación y adición de columnas:\n",
      "                     CE-CO  CE-PM10  julian_day  hour\n",
      "Fecha_Hora                                           \n",
      "2016-01-01 00:00:00   16.0    107.0           1     0\n",
      "2016-01-01 01:00:00   17.0    107.0           1     1\n",
      "2016-01-01 02:00:00   18.0    109.0           1     2\n",
      "2016-01-01 03:00:00   19.0    110.0           1     3\n",
      "2016-01-01 04:00:00   17.0    112.0           1     4\n",
      "\n",
      "Número de valores nulos después de la interpolación:\n",
      "CE-CO         0\n",
      "CE-PM10       0\n",
      "julian_day    0\n",
      "hour          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filtrar el DataFrame para obtener las columnas 'CE-CO' y 'CE-PM10' en el rango de fechas especificado\n",
    "df_impute = df[['CE-CO', 'CE-PM10']].loc['2016-01-01':'2019-05-01']\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame filtrado\n",
    "print(\"Primeras filas del DataFrame filtrado:\")\n",
    "print(df_impute.head())\n",
    "\n",
    "# Graficar las series de tiempo de 'CE-CO' y 'CE-PM10'\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_impute.plot(ax=plt.gca())\n",
    "plt.title('Series de Tiempo de CE-CO y CE-PM10')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Concentración')\n",
    "plt.show()\n",
    "\n",
    "# Contar y mostrar el número de valores nulos en cada columna del DataFrame filtrado\n",
    "print(\"\\nNúmero de valores nulos antes de la imputación:\")\n",
    "print(df_impute.isnull().sum())\n",
    "\n",
    "# Aplicar interpolación para llenar los valores nulos en el DataFrame\n",
    "df_impute.loc[:, :] = df_impute.interpolate(method='time')\n",
    "\n",
    "# Agregar una columna con el día juliano (día del año)\n",
    "df_impute['julian_day'] = df_impute.index.dayofyear\n",
    "\n",
    "# Agregar una columna con la hora del día\n",
    "df_impute['hour'] = df_impute.index.hour\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame después de la interpolación y adición de columnas\n",
    "print(\"\\nPrimeras filas del DataFrame después de la interpolación y adición de columnas:\")\n",
    "print(df_impute.head())\n",
    "\n",
    "# Contar y mostrar el número de valores nulos después de la interpolación\n",
    "print(\"\\nNúmero de valores nulos después de la interpolación:\")\n",
    "print(df_impute.isnull().sum())\n",
    "\n",
    "# Graficar las series de tiempo de 'CE-CO' y 'CE-PM10' después de la interpolación\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_impute[['CE-CO', 'CE-PM10']].plot(ax=plt.gca())\n",
    "plt.title('Series de Tiempo de CE-CO y CE-PM10 (Después de la Interpolación)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Concentración')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del DataFrame filtrado:\n",
      "                     CE-CO  CE-PM10\n",
      "Fecha_Hora                         \n",
      "2016-01-01 00:00:00   16.0    107.0\n",
      "2016-01-01 01:00:00   17.0    107.0\n",
      "2016-01-01 02:00:00   18.0    109.0\n",
      "2016-01-01 03:00:00   19.0    110.0\n",
      "2016-01-01 04:00:00   17.0    112.0\n",
      "\n",
      "Número de valores nulos antes de la imputación:\n",
      "CE-CO      1224\n",
      "CE-PM10       0\n",
      "dtype: int64\n",
      "\n",
      "Primeras filas del DataFrame después del promedio móvil y adición de columnas:\n",
      "                     CE-CO  CE-PM10  julian_day  hour\n",
      "Fecha_Hora                                           \n",
      "2016-01-01 00:00:00   16.0    107.0           1     0\n",
      "2016-01-01 01:00:00   17.0    107.0           1     1\n",
      "2016-01-01 02:00:00   18.0    109.0           1     2\n",
      "2016-01-01 03:00:00   19.0    110.0           1     3\n",
      "2016-01-01 04:00:00   17.0    112.0           1     4\n",
      "\n",
      "Número de valores nulos después del promedio móvil:\n",
      "CE-CO         0\n",
      "CE-PM10       0\n",
      "julian_day    0\n",
      "hour          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filtrar el DataFrame para obtener las columnas 'CE-CO' y 'CE-PM10' en el rango de fechas especificado\n",
    "df_impute = df[['CE-CO', 'CE-PM10']].loc['2016-01-01':'2019-05-01']\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame filtrado\n",
    "print(\"Primeras filas del DataFrame filtrado:\")\n",
    "print(df_impute.head())\n",
    "\n",
    "# Graficar las series de tiempo de 'CE-CO' y 'CE-PM10'\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_impute.plot(ax=plt.gca())\n",
    "plt.title('Series de Tiempo de CE-CO y CE-PM10')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Concentración')\n",
    "plt.show()\n",
    "\n",
    "# Contar y mostrar el número de valores nulos en cada columna del DataFrame filtrado\n",
    "print(\"\\nNúmero de valores nulos antes de la imputación:\")\n",
    "print(df_impute.isnull().sum())\n",
    "\n",
    "# Aplicar el promedio móvil para llenar los valores nulos en el DataFrame\n",
    "df_impute.loc[:, :] = df_impute.fillna(df_impute.rolling(window=12, min_periods=1).mean())\n",
    "\n",
    "# Agregar una columna con el día juliano (día del año)\n",
    "df_impute['julian_day'] = df_impute.index.dayofyear\n",
    "\n",
    "# Agregar una columna con la hora del día\n",
    "df_impute['hour'] = df_impute.index.hour\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame después de la imputación y adición de columnas\n",
    "print(\"\\nPrimeras filas del DataFrame después del promedio móvil y adición de columnas:\")\n",
    "print(df_impute.head())\n",
    "\n",
    "# Contar y mostrar el número de valores nulos después del promedio móvil\n",
    "print(\"\\nNúmero de valores nulos después del promedio móvil:\")\n",
    "print(df_impute.isnull().sum())\n",
    "\n",
    "# Graficar las series de tiempo de 'CE-CO' y 'CE-PM10' después del promedio móvil\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_impute[['CE-CO', 'CE-PM10']].plot(ax=plt.gca())\n",
    "plt.title('Series de Tiempo de CE-CO y CE-PM10 (Después del Promedio Móvil)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Concentración')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Filtrado de valores atipicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Copiar el DataFrame para aplicar diferentes filtros\n",
    "df_filtered_savgol = df_impute.copy()\n",
    "\n",
    "\n",
    "# Aplicar el filtro Savitzky-Golay para suavizar los datos\n",
    "window_length = 13  # El tamaño de la ventana debe ser un número impar\n",
    "polyorder = 3       # El orden del polinomio\n",
    "df_filtered_savgol['CE-CO'] = savgol_filter(df_filtered_savgol['CE-CO'], window_length=window_length, polyorder=polyorder)\n",
    "df_filtered_savgol['CE-PM10'] = savgol_filter(df_filtered_savgol['CE-PM10'], window_length=window_length, polyorder=polyorder)\n",
    "\n",
    "\n",
    "# Graficar las series de tiempo después de aplicar el filtro Savitzky-Golay\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_filtered_savgol[['CE-CO', 'CE-PM10']].plot(ax=plt.gca())\n",
    "plt.title('Series de Tiempo de CE-CO y CE-PM10 (Después del Filtro Savitzky-Golay)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Concentración')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_savgol.to_csv('Datos_procesados.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando Redes Neuronales Recurrentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula el error cuadrático medio (RMSE) entre las predicciones y los valores verdaderos.\n",
    "\n",
    "    Args:\n",
    "        y_true (tensor): Valores verdaderos.\n",
    "        y_pred (tensor): Valores predichos.\n",
    "\n",
    "    Returns:\n",
    "        tensor: RMSE.\n",
    "    \"\"\"\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def split_dataframe(df, train_ratio=0.7, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Divide un DataFrame en conjuntos de entrenamiento, prueba y evaluación.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): El DataFrame a dividir.\n",
    "        train_ratio (float, optional): Proporción de datos para entrenamiento. Defaults to 0.7.\n",
    "        test_ratio (float, optional): Proporción de datos para prueba. Defaults to 0.2.\n",
    "\n",
    "    Returns:\n",
    "        tuple: DataFrames de entrenamiento, prueba y evaluación.\n",
    "    \"\"\"\n",
    "    total_size = len(df)  # Tamaño total del DataFrame\n",
    "    train_size = int(total_size * train_ratio)  # Tamaño del conjunto de entrenamiento\n",
    "    test_size = int(total_size * test_ratio)  # Tamaño del conjunto de prueba\n",
    "    eval_size = total_size - train_size - test_size  # Tamaño del conjunto de evaluación\n",
    "    \n",
    "    df_train = df.iloc[:train_size]  # Conjunto de entrenamiento\n",
    "    df_test = df.iloc[train_size:train_size + test_size]  # Conjunto de prueba\n",
    "    df_eval = df.iloc[train_size + test_size:]  # Conjunto de evaluación\n",
    "    \n",
    "    return df_train, df_test, df_eval\n",
    "\n",
    "\n",
    "def scale_datasets(train_df, test_df, eval_df):\n",
    "    \"\"\"\n",
    "    Escala los conjuntos de datos de entrenamiento, prueba y evaluación.\n",
    "\n",
    "    Args:\n",
    "        train_df (DataFrame): DataFrame de entrenamiento.\n",
    "        test_df (DataFrame): DataFrame de prueba.\n",
    "        eval_df (DataFrame): DataFrame de evaluación.\n",
    "\n",
    "    Returns:\n",
    "        tuple: DataFrames escalados de entrenamiento, prueba y evaluación, y diccionario de escaladores.\n",
    "    \"\"\"\n",
    "    scalers = {}  # Diccionario para almacenar los escaladores\n",
    "    scaled_train_df = train_df.copy()  # Copia del DataFrame de entrenamiento\n",
    "    scaled_test_df = test_df.copy()  # Copia del DataFrame de prueba\n",
    "    scaled_eval_df = eval_df.copy()  # Copia del DataFrame de evaluación\n",
    "    \n",
    "    for column in train_df.columns:\n",
    "        scaler = MinMaxScaler()  # Crear un escalador MinMax para cada columna\n",
    "        scaled_train_df[column] = scaler.fit_transform(train_df[[column]])  # Ajustar y transformar los datos de entrenamiento\n",
    "        scaled_test_df[column] = scaler.transform(test_df[[column]])  # Transformar los datos de prueba\n",
    "        scaled_eval_df[column] = scaler.transform(eval_df[[column]])  # Transformar los datos de evaluación\n",
    "        scalers[column] = scaler  # Guardar el escalador para cada columna\n",
    "        \n",
    "    return scaled_train_df, scaled_test_df, scaled_eval_df, scalers\n",
    "\n",
    "def create_sequences(data, input_columns, output_columns, time_steps, future_steps=1):\n",
    "    \"\"\"\n",
    "    Crea secuencias de datos para modelado de series temporales.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame con los datos.\n",
    "        input_columns (list): Columnas de entrada.\n",
    "        output_columns (list): Columnas de salida.\n",
    "        time_steps (int): Número de pasos de tiempo para las secuencias de entrada.\n",
    "        future_steps (int, optional): Número de pasos de tiempo futuros para las secuencias de salida. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Arrays de entrada y salida para el modelo.\n",
    "    \"\"\"\n",
    "    X, y = [], []  # Listas para almacenar las secuencias de entrada y salida\n",
    "    data_array = data.values  # Convertir el DataFrame a un array numpy\n",
    "\n",
    "    input_indices = data.columns.get_indexer(input_columns)  # Índices de las columnas de entrada\n",
    "    output_indices = data.columns.get_indexer(output_columns)  # Índices de las columnas de salida\n",
    "\n",
    "    for i in range(len(data) - time_steps - future_steps + 1):\n",
    "        X.append(data_array[i:i + time_steps, input_indices])  # Crear secuencia de entrada\n",
    "        y.append(data_array[i + time_steps:i + time_steps + future_steps, output_indices].flatten())  # Crear secuencia de salida\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_simple_rnn_model(input_shape, output_size, future_steps, rnn_units=50, hidden_units=50, output_activation='linear'):\n",
    "    \"\"\"\n",
    "    Crea un modelo SimpleRNN.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Forma de las secuencias de entrada.\n",
    "        output_size (int): Tamaño de la salida.\n",
    "        future_steps (int): Número de pasos de tiempo futuros.\n",
    "        rnn_units (int, optional): Unidades de la capa RNN. Defaults to 50.\n",
    "        hidden_units (int, optional): Unidades de la capa oculta. Defaults to 50.\n",
    "        output_activation (str, optional): Activación de la capa de salida. Defaults to 'linear'.\n",
    "\n",
    "    Returns:\n",
    "        model: Modelo compilado de SimpleRNN.\n",
    "    \"\"\"\n",
    "    model = Sequential()  # Crear un modelo secuencial\n",
    "    model.add(SimpleRNN(rnn_units, input_shape=input_shape, return_sequences=False))  # Añadir capa SimpleRNN\n",
    "    model.add(Dense(hidden_units, activation='relu'))  # Añadir capa oculta densa\n",
    "    model.add(Dense(output_size * future_steps, activation=output_activation))  # Añadir capa de salida\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[rmse])  # Compilar el modelo\n",
    "    return model\n",
    "\n",
    "def create_lstm_model(input_shape, output_size, future_steps, lstm_units=50, hidden_units=50, output_activation='linear'):\n",
    "    \"\"\"\n",
    "    Crea un modelo LSTM.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Forma de las secuencias de entrada.\n",
    "        output_size (int): Tamaño de la salida.\n",
    "        future_steps (int): Número de pasos de tiempo futuros.\n",
    "        lstm_units (int, optional): Unidades de la capa LSTM. Defaults to 50.\n",
    "        hidden_units (int, optional): Unidades de la capa oculta. Defaults to 50.\n",
    "        output_activation (str, optional): Activación de la capa de salida. Defaults to 'linear'.\n",
    "\n",
    "    Returns:\n",
    "        model: Modelo compilado de LSTM.\n",
    "    \"\"\"\n",
    "    model = Sequential()  # Crear un modelo secuencial\n",
    "    model.add(LSTM(lstm_units, input_shape=input_shape, return_sequences=False))  # Añadir capa LSTM\n",
    "    model.add(Dense(hidden_units, activation='relu'))  # Añadir capa oculta densa\n",
    "    model.add(Dense(output_size * future_steps, activation=output_activation))  # Añadir capa de salida\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[rmse])  # Compilar el modelo\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_gru_model(input_shape, output_size, future_steps, gru_units=50, hidden_units=50, output_activation='linear'):\n",
    "    \"\"\"\n",
    "    Crea un modelo GRU.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Forma de las secuencias de entrada.\n",
    "        output_size (int): Tamaño de la salida.\n",
    "        future_steps (int): Número de pasos de tiempo futuros.\n",
    "        gru_units (int, optional): Unidades de la capa GRU. Defaults to 50.\n",
    "        hidden_units (int, optional): Unidades de la capa oculta. Defaults to 50.\n",
    "        output_activation (str, optional): Activación de la capa de salida. Defaults to 'linear'.\n",
    "\n",
    "    Returns:\n",
    "        model: Modelo compilado de GRU.\n",
    "    \"\"\"\n",
    "    model = Sequential()  # Crear un modelo secuencial\n",
    "    model.add(GRU(gru_units, input_shape=input_shape, return_sequences=False))  # Añadir capa GRU\n",
    "    model.add(Dense(hidden_units, activation='relu'))  # Añadir capa oculta densa\n",
    "    model.add(Dense(output_size * future_steps, activation=output_activation))  # Añadir capa de salida\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[rmse])  # Compilar el modelo\n",
    "    return model\n",
    "\n",
    "def plot_final_prediction(model, X, y, scalers, output_columns, future_steps, title):\n",
    "    \"\"\"\n",
    "    Grafica las predicciones finales del modelo en comparación con los valores reales.\n",
    "\n",
    "    Args:\n",
    "        model (Sequential): Modelo entrenado.\n",
    "        X (ndarray): Datos de entrada.\n",
    "        y (ndarray): Valores reales.\n",
    "        scalers (dict): Diccionario de escaladores para reescalar los datos.\n",
    "        output_columns (list): Columnas de salida.\n",
    "        future_steps (int): Número de pasos de tiempo futuros.\n",
    "        title (str): Título de la gráfica.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X).reshape(-1, future_steps, len(output_columns))  # Realizar predicciones y darles forma\n",
    "    \n",
    "    # Reescalar las predicciones y los valores reales\n",
    "    y_rescaled = np.zeros((y.shape[0], len(output_columns)))\n",
    "    predictions_rescaled = np.zeros((predictions.shape[0], future_steps, len(output_columns)))\n",
    "    \n",
    "    for i, column in enumerate(output_columns):\n",
    "        y_rescaled[:, i] = scalers[column].inverse_transform(y[:, i].reshape(-1, 1)).reshape(-1)  # Reescalar valores reales\n",
    "        for step in range(future_steps):\n",
    "            predictions_rescaled[:, step, i] = scalers[column].inverse_transform(predictions[:, step, i].reshape(-1, 1)).reshape(-1)  # Reescalar predicciones\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))  # Crear figura\n",
    "    for i in range(len(output_columns)):\n",
    "        plt.plot(y_rescaled[:, i], label='Valor Real')  # Graficar valores reales\n",
    "        plt.plot(predictions_rescaled[:, -1, i], label='Predicción')  # Graficar predicciones\n",
    "        plt.title(f'{title} - Variable {output_columns[i]}')  # Añadir título\n",
    "        plt.legend()  \n",
    "    \n",
    "    plt.tight_layout() \n",
    "    plt.show() \n",
    "\n",
    "# Función para graficar los errores finales del modelo\n",
    "def plot_final_errors(model, X, y, scalers, output_columns, future_steps, title):\n",
    "    \"\"\"\n",
    "    Grafica los errores finales del modelo en comparación con los valores reales.\n",
    "\n",
    "    Args:\n",
    "        model (Sequential): Modelo entrenado.\n",
    "        X (ndarray): Datos de entrada.\n",
    "        y (ndarray): Valores reales.\n",
    "        scalers (dict): Diccionario de escaladores para reescalar los datos.\n",
    "        output_columns (list): Columnas de salida.\n",
    "        future_steps (int): Número de pasos de tiempo futuros.\n",
    "        title (str): Título de la gráfica.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X).reshape(-1, future_steps, len(output_columns))  # Realizar predicciones y darles forma\n",
    "    \n",
    "    # Reescalar las predicciones y los valores reales\n",
    "    y_rescaled = np.zeros((y.shape[0], len(output_columns)))\n",
    "    predictions_rescaled = np.zeros((predictions.shape[0], future_steps, len(output_columns)))\n",
    "    \n",
    "    for i, column in enumerate(output_columns):\n",
    "        y_rescaled[:, i] = scalers[column].inverse_transform(y[:, i].reshape(-1, 1)).reshape(-1)  # Reescalar valores reales\n",
    "        for step in range(future_steps):\n",
    "            predictions_rescaled[:, step, i] = scalers[column].inverse_transform(predictions[:, step, i].reshape(-1, 1)).reshape(-1)  # Reescalar predicciones\n",
    "    \n",
    "    errors_rescaled = y_rescaled - predictions_rescaled[:, -1, :]  # Calcular errores reescalados\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))  # Crear figura\n",
    "    for i in range(len(output_columns)):\n",
    "        plt.plot(errors_rescaled[:, i], label='Error')  # Graficar errores\n",
    "        plt.title(f'{title} - Error Variable {output_columns[i]}')  # Añadir título\n",
    "        plt.legend() \n",
    "    \n",
    "    plt.tight_layout()  \n",
    "    plt.show()\n",
    "\n",
    "def plot_training_history(history, title='Historial de Entrenamiento'):\n",
    "    \"\"\"\n",
    "    Grafica el historial de entrenamiento del modelo.\n",
    "\n",
    "    Args:\n",
    "        history (History): Historial de entrenamiento del modelo.\n",
    "        title (str, optional): Título de la gráfica. Defaults to 'Historial de Entrenamiento'.\n",
    "    \"\"\"\n",
    "    # Extraer la información del historial\n",
    "    history_dict = history.history\n",
    "    \n",
    "    # Crear una figura\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Graficar la pérdida (loss)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history_dict['loss'], label='Pérdida en Entrenamiento')\n",
    "    if 'val_loss' in history_dict:\n",
    "        plt.plot(history_dict['val_loss'], label='Pérdida en Validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.title('Pérdida durante el Entrenamiento')\n",
    "    plt.legend()\n",
    "\n",
    "    # Graficar la métrica RMSE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_dict['rmse'], label='RMSE en Entrenamiento')\n",
    "    if 'val_rmse' in history_dict:\n",
    "        plt.plot(history_dict['val_rmse'], label='RMSE en Validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('RMSE durante el Entrenamiento')\n",
    "    plt.legend()\n",
    "\n",
    "    # Ajustar el diseño y mostrar la gráfica\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Leer el archivo CSV\n",
    "file_path = 'Datos_procesados.csv'  # Reemplaza con la ruta a tu archivo\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df_train, df_test, df_eval = split_dataframe(df, train_ratio=0.7, test_ratio=0.2)\n",
    "\n",
    "print(f\"Tamaño de datos de entrenamiento: {len(df_train)}\")\n",
    "print(f\"Tamaño de datos de prueba: {len(df_test)}\")\n",
    "print(f\"Tamaño de datos de evaluación: {len(df_eval)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_df, scaled_test_df, scaled_eval_df, scalers = scale_datasets(df_train, df_test, df_eval)\n",
    "\n",
    "time_steps = 24\n",
    "future_steps = 12\n",
    "#input_columns = ['CE-CO','CE-PM10','julian_day','hour']  # Nombres de las columnas de entrada en el DataFrame escalado\n",
    "input_columns = ['CE-CO']  # Nombres de las columnas de entrada en el DataFrame escalado\n",
    "output_columns = ['CE-CO']  # Nombres de las columnas de salida en el DataFrame escalado\n",
    "\n",
    "\n",
    "X_train, y_train = create_sequences(scaled_train_df, input_columns, output_columns, time_steps, future_steps)\n",
    "X_test, y_test = create_sequences(scaled_test_df, input_columns, output_columns, time_steps, future_steps)\n",
    "X_eval, y_eval = create_sequences(scaled_eval_df, input_columns, output_columns, time_steps, future_steps)\n",
    "\n",
    "print(f\"Tamaño de X_train: {X_train.shape}\")\n",
    "print(f\"Tamaño de y_train: {y_train.shape}\")\n",
    "print(f\"Tamaño de X_test: {X_test.shape}\")\n",
    "print(f\"Tamaño de y_test: {y_test.shape}\")\n",
    "print(f\"Tamaño de X_eval: {X_eval.shape}\")\n",
    "print(f\"Tamaño de y_eval: {y_eval.shape}\")\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "output_size = len(output_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar el modelo SimpleRNN\n",
    "model_simple_rnn = create_simple_rnn_model(input_shape, output_size, future_steps)\n",
    "history_simple_rnn = model_simple_rnn.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test), shuffle=True)\n",
    "plot_training_history(history_simple_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar el modelo LSTM\n",
    "model_lstm = create_lstm_model(input_shape, output_size, future_steps)\n",
    "history_lstm = model_lstm.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test), shuffle=True)\n",
    "plot_training_history(history_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar el modelo GRU\n",
    "model_gru = create_gru_model(input_shape, output_size, future_steps)\n",
    "history_gru = model_gru.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test), shuffle=True)\n",
    "plot_training_history(history_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar los modelos en el conjunto de evaluación\n",
    "loss_simple_rnn, rmse_simple_rnn = model_simple_rnn.evaluate(X_eval, y_eval)\n",
    "print(f\"SimpleRNN - Loss en el conjunto de evaluación: {loss_simple_rnn}\")\n",
    "print(f\"SimpleRNN - RMSE en el conjunto de evaluación: {rmse_simple_rnn}\")\n",
    "\n",
    "loss_lstm, rmse_lstm = model_lstm.evaluate(X_eval, y_eval)\n",
    "print(f\"LSTM - Loss en el conjunto de evaluación: {loss_lstm}\")\n",
    "print(f\"LSTM - RMSE en el conjunto de evaluación: {rmse_lstm}\")\n",
    "\n",
    "loss_gru, rmse_gru = model_gru.evaluate(X_eval, y_eval)\n",
    "print(f\"GRU - Loss en el conjunto de evaluación: {loss_gru}\")\n",
    "print(f\"GRU - RMSE en el conjunto de evaluación: {rmse_gru}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_final_prediction(model_simple_rnn, X_eval, y_eval, scalers, output_columns, future_steps, title='SimpleRNN - Predicción Final')\n",
    "plot_final_errors(model_simple_rnn, X_eval, y_eval, scalers, output_columns, future_steps, title='SimpleRNN - Error Final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar predicciones y errores para el modelo LSTM\n",
    "plot_final_prediction(model_lstm, X_eval, y_eval, scalers, output_columns, future_steps, title='LSTM - Predicciones en el conjunto de evaluación')\n",
    "plot_final_errors(model_lstm, X_eval, y_eval, scalers, output_columns, future_steps, title='LSTM - Errores en el conjunto de evaluación')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualizar predicciones y errores para el modelo GRU\n",
    "plot_final_prediction(model_gru, X_eval, y_eval, scalers, output_columns, future_steps, title='GRU - Predicciones en el conjunto de evaluación')\n",
    "plot_final_errors(model_gru, X_eval, y_eval, scalers, output_columns, future_steps, title='GRU - Errores en el conjunto de evaluación')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_predictions(model, data, input_columns, output_columns, time_steps, future_steps):\n",
    "    \"\"\"\n",
    "    Añade múltiples predicciones al DataFrame de una sola vez.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado.\n",
    "        data (pd.DataFrame): DataFrame con los datos originales.\n",
    "        input_columns (list of str): Lista de nombres de las columnas de entrada.\n",
    "        output_columns (list of str): Lista de nombres de las columnas de salida.\n",
    "        time_steps (int): Número de pasos de tiempo para las secuencias.\n",
    "        future_steps (int): Número de pasos de tiempo futuros a predecir.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las predicciones añadidas.\n",
    "    \"\"\"\n",
    "    # Escaladores\n",
    "    scalers = {col: MinMaxScaler() for col in data.columns}\n",
    "    \n",
    "    # Escalar datos\n",
    "    scaled_data = data.copy()\n",
    "    for col in data.columns:\n",
    "        scaled_data[col] = scalers[col].fit_transform(data[[col]])\n",
    "    \n",
    "    # Seleccionar la última secuencia de time_steps\n",
    "    last_seq = scaled_data[input_columns].values[-time_steps:]\n",
    "    last_seq = np.expand_dims(last_seq, axis=0)\n",
    "    \n",
    "    # Realizar la predicción\n",
    "    predictions_scaled = model.predict(last_seq)\n",
    "    \n",
    "    # Reescalar las predicciones\n",
    "    predictions = np.zeros((future_steps, len(output_columns)))\n",
    "    for i, col in enumerate(output_columns):\n",
    "        predictions[:, i] = scalers[col].inverse_transform(predictions_scaled[:, i::len(output_columns)].reshape(-1, 1)).reshape(-1)\n",
    "    \n",
    "    # Crear un DataFrame para las predicciones\n",
    "    predictions_df = pd.DataFrame(predictions, columns=output_columns)\n",
    "    \n",
    "    # Añadir las predicciones al DataFrame original\n",
    "    data_with_predictions = pd.concat([data, predictions_df], ignore_index=True)\n",
    "    \n",
    "    return data_with_predictions\n",
    "\n",
    "def plot_with_predictions(data, output_columns, plot_steps, future_steps):\n",
    "    \"\"\"\n",
    "    Visualiza los datos actuales y las predicciones.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame con los datos actuales y las predicciones.\n",
    "        output_columns (list of str): Lista de nombres de las columnas de salida.\n",
    "        plot_steps (int): Número de pasos de tiempo a graficar antes de la predicción.\n",
    "        future_steps (int): Número de pasos de tiempo futuros a predecir.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    for col in output_columns:\n",
    "        plt.plot(data.index[-(plot_steps + future_steps):], data[col].iloc[-(plot_steps + future_steps):], label=f'Actual {col}')\n",
    "        plt.plot(data.index[-future_steps:], data[col].iloc[-future_steps:], 'ro-', label=f'Predicción {col}')\n",
    "    \n",
    "    plt.title('Predicciones y datos actuales')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_steps = 12  # Número de pasos de tiempo a graficar antes de la predicción\n",
    "\n",
    "# Añadir predicciones y visualizar\n",
    "df_with_predictions = add_predictions(model_simple_rnn, df, input_columns, output_columns, time_steps, future_steps)\n",
    "plot_with_predictions(df_with_predictions, output_columns, plot_steps, future_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple_rnn.save('model_rnn.keras')\n",
    "model_gru.save('model_gru.keras')\n",
    "model_lstm.save('model_lstm.keras')\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def save_scalers(scalers, directory):\n",
    "    \"\"\"\n",
    "    Guarda los escaladores en archivos separados.\n",
    "\n",
    "    Args:\n",
    "        scalers (dict): Diccionario de escaladores.\n",
    "        directory (str): Directorio donde se guardarán los escaladores.\n",
    "    \"\"\"\n",
    "    # Crear el directorio si no existe\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Guardar cada escalador en un archivo separado\n",
    "    for column, scaler in scalers.items():\n",
    "        file_path = os.path.join(directory, f'{column}_scaler.pkl')\n",
    "        joblib.dump(scaler, file_path)\n",
    "        print(f\"Escalador para {column} guardado en {file_path}\")\n",
    "\n",
    "\n",
    "save_scalers(scalers, 'scalers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "file_path = 'Datos_procesados.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Definición de funciones\n",
    "\n",
    "def load_scalers(directory, columns):\n",
    "    \"\"\"\n",
    "    Carga los escaladores desde archivos separados.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Directorio desde donde se cargarán los escaladores.\n",
    "        columns (list): Lista de nombres de columnas para las cuales se cargan los escaladores.\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario de escaladores cargados.\n",
    "    \"\"\"\n",
    "    scalers = {}\n",
    "    \n",
    "    # Cargar cada escalador desde su archivo\n",
    "    for column in columns:\n",
    "        file_path = os.path.join(directory, f'{column}_scaler.pkl')\n",
    "        if os.path.exists(file_path):\n",
    "            scalers[column] = joblib.load(file_path)\n",
    "            print(f\"Escalador para {column} cargado desde {file_path}\")\n",
    "        else:\n",
    "            print(f\"No se encontró el archivo {file_path} para {column}\")\n",
    "    \n",
    "    return scalers\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula el error cuadrático medio (RMSE) entre las predicciones y los valores verdaderos.\n",
    "\n",
    "    Args:\n",
    "        y_true (tensor): Valores verdaderos.\n",
    "        y_pred (tensor): Valores predichos.\n",
    "\n",
    "    Returns:\n",
    "        tensor: RMSE.\n",
    "    \"\"\"\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def add_predictions(model, data, input_columns, output_columns, time_steps, future_steps, scalers):\n",
    "    \"\"\"\n",
    "    Añade múltiples predicciones al DataFrame de una sola vez.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado.\n",
    "        data (pd.DataFrame): DataFrame con los datos originales.\n",
    "        input_columns (list of str): Lista de nombres de las columnas de entrada.\n",
    "        output_columns (list of str): Lista de nombres de las columnas de salida.\n",
    "        time_steps (int): Número de pasos de tiempo para las secuencias.\n",
    "        future_steps (int): Número de pasos de tiempo futuros a predecir.\n",
    "        scalers (dict): Diccionario de escaladores.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las predicciones añadidas.\n",
    "    \"\"\"\n",
    "    # Escalar datos\n",
    "    scaled_data = data.copy()\n",
    "    for col in data.columns:\n",
    "        scaled_data[col] = scalers[col].transform(data[[col]])\n",
    "    \n",
    "    # Seleccionar la última secuencia de time_steps\n",
    "    last_seq = scaled_data[input_columns].values[-time_steps:]\n",
    "    last_seq = np.expand_dims(last_seq, axis=0)\n",
    "    \n",
    "    # Realizar la predicción\n",
    "    predictions_scaled = model.predict(last_seq)\n",
    "    \n",
    "    # Reescalar las predicciones\n",
    "    predictions = np.zeros((future_steps, len(output_columns)))\n",
    "    for i, col in enumerate(output_columns):\n",
    "        predictions[:, i] = scalers[col].inverse_transform(predictions_scaled[:, i::len(output_columns)].reshape(-1, 1)).reshape(-1)\n",
    "    \n",
    "    # Crear un DataFrame para las predicciones\n",
    "    predictions_df = pd.DataFrame(predictions, columns=output_columns)\n",
    "    \n",
    "    # Añadir las predicciones al DataFrame original\n",
    "    data_with_predictions = pd.concat([data, predictions_df], ignore_index=True)\n",
    "    \n",
    "    return data_with_predictions\n",
    "\n",
    "def plot_with_predictions(data, output_columns, plot_steps, future_steps):\n",
    "    \"\"\"\n",
    "    Visualiza los datos actuales y las predicciones.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame con los datos actuales y las predicciones.\n",
    "        output_columns (list of str): Lista de nombres de las columnas de salida.\n",
    "        plot_steps (int): Número de pasos de tiempo a graficar antes de la predicción.\n",
    "        future_steps (int): Número de pasos de tiempo futuros a predecir.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    for col in output_columns:\n",
    "        plt.plot(data.index[-(plot_steps + future_steps):], data[col].iloc[-(plot_steps + future_steps):], label=f'Actual {col}')\n",
    "        plt.plot(data.index[-future_steps:], data[col].iloc[-future_steps:], 'ro-', label=f'Predicción {col}')\n",
    "    \n",
    "    plt.title('Predicciones y datos actuales')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Cargar los escaladores\n",
    "columns = df.columns\n",
    "loaded_scalers = load_scalers('scalers', columns)\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "model = tf.keras.models.load_model('model_rnn.keras', custom_objects={'rmse': rmse})\n",
    "\n",
    "# Definir parámetros\n",
    "time_steps = 24\n",
    "future_steps = 12\n",
    "input_columns = ['CE-CO', 'CE-PM10', 'julian_day', 'hour']\n",
    "output_columns = ['CE-CO']\n",
    "plot_steps = 12  # Número de pasos de tiempo a graficar antes de la predicción\n",
    "\n",
    "# Añadir predicciones y visualizar\n",
    "df_with_predictions = add_predictions(model, df, input_columns, output_columns, time_steps, future_steps, loaded_scalers)\n",
    "plot_with_predictions(df_with_predictions, output_columns, plot_steps, future_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En desarrollo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_nowcast(data, window=12):\n",
    "    \"\"\"\n",
    "    Calcula el nowcast basado en una ventana móvil sobre los datos de concentración.\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series): Serie temporal de datos de concentración.\n",
    "        window (int, optional): Tamaño de la ventana móvil. Por defecto es 12.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Arreglo con los valores de nowcast calculados.\n",
    "    \"\"\"\n",
    "    # Inicializar el arreglo de nowcast con NaN\n",
    "    nowcast = np.full(len(data), np.nan, dtype=np.float16)\n",
    "    \n",
    "    # Iterar sobre los datos comenzando desde el índice 'window - 1'\n",
    "    for i in range(window - 1, len(data)):\n",
    "        # Obtener los datos de la ventana actual\n",
    "        window_data = data.iloc[i - window + 1:i + 1].to_numpy()\n",
    "        \n",
    "        # Calcular el valor máximo y mínimo de la ventana\n",
    "        C_max = np.nanmax(window_data)\n",
    "        C_min = np.nanmin(window_data)\n",
    "        \n",
    "        # Calcular el peso 'w' basado en la diferencia entre el máximo y el mínimo\n",
    "        w = 1 - (C_max - C_min) / C_max if C_max != 0 else 0\n",
    "        \n",
    "        # Ajustar el peso 'W' para que no sea menor a 0.5\n",
    "        W = w if w > 0.5 else 0.5\n",
    "        \n",
    "        # Calcular los pesos para cada valor en la ventana\n",
    "        weights = np.where(~np.isnan(window_data), W ** np.arange(window), 0)\n",
    "        \n",
    "        # Sumar los pesos, ignorando los NaN\n",
    "        weights_sum = np.nansum(weights)\n",
    "        \n",
    "        # Si en los últimos tres valores de la ventana hay como máximo dos NaN\n",
    "        if np.count_nonzero(np.isnan(window_data[-3:])) <= 2:\n",
    "            # Calcular el nowcast como la media ponderada de los valores en la ventana\n",
    "            nowcast[i] = np.round(np.nansum(window_data * weights) / weights_sum)\n",
    "    \n",
    "    return nowcast\n",
    "\n",
    "# Lista de concentraciones\n",
    "concentraciones = [13, np.nan, 10, 21, 74, 64, 53, 82, 90, 75, 80, 50]\n",
    "\n",
    "# Convertir la lista en una Serie de pandas\n",
    "concentraciones_series = pd.Series(concentraciones)\n",
    "\n",
    "# Aplicar la función calculate_nowcast\n",
    "nowcast_result = calculate_nowcast(concentraciones_series)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(nowcast_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Crear un DataFrame ficticio\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'Fecha': pd.date_range(start='2022-01-01', periods=100, freq='D'),\n",
    "    'Variable1': np.random.randn(100),\n",
    "    'Variable2': np.random.randn(100),\n",
    "    'Variable3': np.random.randn(100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introducir valores faltantes\n",
    "df.loc[df.sample(frac=0.1).index, 'Variable1'] = np.nan\n",
    "df.loc[df.sample(frac=0.1).index, 'Variable2'] = np.nan\n",
    "df.loc[df.sample(frac=0.1).index, 'Variable3'] = np.nan\n",
    "\n",
    "print(df.head(10))\n",
    "df_media = df.copy()\n",
    "df_media['Variable1'] = df_media['Variable1'].fillna(df_media['Variable1'].mean())\n",
    "df_media['Variable2'] = df_media['Variable2'].fillna(df_media['Variable2'].mean())\n",
    "df_media['Variable3'] = df_media['Variable3'].fillna(df_media['Variable3'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "df_knn = df.copy()\n",
    "imputer_knn = KNNImputer(n_neighbors=5)\n",
    "df_knn.iloc[:, 1:] = imputer_knn.fit_transform(df_knn.iloc[:, 1:])\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "df_mice = df.copy()\n",
    "imputer_mice = IterativeImputer()\n",
    "df_mice.iloc[:, 1:] = imputer_mice.fit_transform(df_mice.iloc[:, 1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_statistics(df, method):\n",
    "    summary = df.describe().T[['mean', 'std']]\n",
    "    summary['method'] = method\n",
    "    return summary\n",
    "\n",
    "summary_original = summary_statistics(df.dropna(), 'Original')\n",
    "summary_media = summary_statistics(df_media, 'Media')\n",
    "summary_knn = summary_statistics(df_knn, 'KNN')\n",
    "summary_mice = summary_statistics(df_mice, 'MICE')\n",
    "\n",
    "summary_df = pd.concat([summary_original, summary_media, summary_knn, summary_mice])\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
